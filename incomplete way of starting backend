# FastAPI Production-Ready Deployment Guide

## Phase 1: Project Setup & Environment

### Step 1 - Initialize Project Structure
```
my_demo_app/
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pre-commit-config.yaml
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ alembic.ini
â”œâ”€â”€ alembic/
â”‚   â”œâ”€â”€ env.py
â”‚   â””â”€â”€ versions/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # FastAPI app
â”‚   â”œâ”€â”€ config.py               # Settings & environment
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ connection.py       # DB pool
â”‚   â”‚   â””â”€â”€ queries.py          # Raw SQL queries
â”‚   â”œâ”€â”€ cache/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ redis_client.py     # Redis operations
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ endpoints/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ products.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ communes.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ companies.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ health.py
â”‚   â”‚   â”‚   â””â”€â”€ router.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ requests.py         # Pydantic request models
â”‚   â”‚   â”œâ”€â”€ responses.py        # Pydantic response models
â”‚   â”‚   â””â”€â”€ errors.py           # Error models
â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ cors.py
â”‚   â”‚   â”œâ”€â”€ logging.py
â”‚   â”‚   â””â”€â”€ security.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ content_moderation.py
â”‚   â”‚   â”œâ”€â”€ security.py
â”‚   â”‚   â””â”€â”€ validators.py
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ conftest.py
â”‚       â”œâ”€â”€ test_endpoints.py
â”‚       â”œâ”€â”€ test_database.py
â”‚       â””â”€â”€ test_cache.py
â””â”€â”€ scripts/
    â”œâ”€â”€ setup.sh
    â”œâ”€â”€ migrate.sh
    â””â”€â”€ deploy.sh
```

### Step 2 - Install Dependencies
```bash
# requirements.txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
gunicorn==21.2.0
asyncpg==0.29.0
alembic==1.12.1
aioredis==2.0.1
pydantic[email]==2.5.0
pydantic-settings==2.1.0
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-multipart==0.0.6
better-profanity==0.7.0
python-magic==0.4.27
structlog==23.2.0

# requirements-dev.txt
pytest==7.4.3
pytest-asyncio==0.21.1
httpx==0.25.2
black==23.11.0
isort==5.12.0
flake8==6.1.0
mypy==1.7.1
pre-commit==3.5.0
locust==2.17.0
```

### Step 3 - Environment Configuration
```python
# app/config.py
from pydantic_settings import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    # Database
    database_url: str
    db_pool_min_size: int = 5
    db_pool_max_size: int = 20
    db_timeout: int = 30
    
    # Redis
    redis_url: str
    redis_timeout: int = 5
    cache_ttl: int = 3600
    
    # Security
    secret_key: str
    algorithm: str = "HS256"
    access_token_expire_minutes: int = 30
    
    # Content Moderation
    enable_content_moderation: bool = True
    max_file_size: int = 10_000_000  # 10MB
    allowed_file_types: list[str] = ["image/jpeg", "image/png", "image/webp"]
    
    # API
    api_v1_prefix: str = "/api/v1"
    project_name: str = "Demo API"
    debug: bool = False
    
    # CORS
    allowed_origins: list[str] = ["http://localhost:3000"]
    
    class Config:
        env_file = ".env"

settings = Settings()
```

## Phase 2: Database & Migrations

### Step 4 - Database Connection with Pooling
```python
# app/database/connection.py
import asyncpg
import structlog
from typing import AsyncGenerator
from app.config import settings

logger = structlog.get_logger()
pool: asyncpg.Pool = None

async def init_db_pool():
    global pool
    try:
        pool = await asyncpg.create_pool(
            dsn=settings.database_url,
            min_size=settings.db_pool_min_size,
            max_size=settings.db_pool_max_size,
            command_timeout=settings.db_timeout,
        )
        logger.info("Database pool initialized")
    except Exception as e:
        logger.error("Failed to initialize database pool", error=str(e))
        raise

async def close_db_pool():
    global pool
    if pool:
        await pool.close()
        logger.info("Database pool closed")

async def get_db() -> AsyncGenerator[asyncpg.Connection, None]:
    async with pool.acquire() as conn:
        try:
            async with conn.transaction():
                yield conn
        except Exception as e:
            logger.error("Database transaction failed", error=str(e))
            raise
```

### Step 5 - Alembic Migration with Constraints & Indexes
```python
# alembic/versions/001_initial_tables.py
from alembic import op
import sqlalchemy as sa

revision = '001_initial'
down_revision = None

def upgrade():
    # Products table
    op.execute("""
        CREATE TABLE products (
            uuid UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            name_es TEXT NOT NULL CHECK (char_length(name_es) > 0),
            name_en TEXT NOT NULL CHECK (char_length(name_en) > 0),
            created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
            updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
            deleted_at TIMESTAMPTZ NULL
        );
        
        CREATE INDEX idx_products_name_es ON products(name_es) WHERE deleted_at IS NULL;
        CREATE INDEX idx_products_created_at ON products(created_at) WHERE deleted_at IS NULL;
    """)
    
    # Communes table
    op.execute("""
        CREATE TABLE communes (
            uuid UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            name TEXT NOT NULL UNIQUE CHECK (char_length(name) > 0),
            created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
            updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
            deleted_at TIMESTAMPTZ NULL
        );
        
        CREATE INDEX idx_communes_name ON communes(name) WHERE deleted_at IS NULL;
    """)
    
    # Companies table
    op.execute("""
        CREATE TABLE companies (
            uuid UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            user_uuid UUID NOT NULL,
            product_uuid UUID NOT NULL,
            commune_uuid UUID NOT NULL,
            name TEXT NOT NULL CHECK (char_length(name) BETWEEN 1 AND 255),
            description_es TEXT CHECK (char_length(description_es) <= 2000),
            description_en TEXT CHECK (char_length(description_en) <= 2000),
            address TEXT CHECK (char_length(address) <= 500),
            phone TEXT CHECK (phone ~ '^[0-9+\-\s()]+$'),
            email TEXT CHECK (email ~* '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$'),
            image_url TEXT,
            created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
            updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
            deleted_at TIMESTAMPTZ NULL,
            
            FOREIGN KEY (product_uuid) REFERENCES products(uuid),
            FOREIGN KEY (commune_uuid) REFERENCES communes(uuid)
        );
        
        CREATE INDEX idx_companies_product_uuid ON companies(product_uuid) WHERE deleted_at IS NULL;
        CREATE INDEX idx_companies_commune_uuid ON companies(commune_uuid) WHERE deleted_at IS NULL;
        CREATE INDEX idx_companies_created_at ON companies(created_at) WHERE deleted_at IS NULL;
        CREATE INDEX idx_companies_name ON companies USING gin(to_tsvector('spanish', name)) WHERE deleted_at IS NULL;
    """)

def downgrade():
    op.execute("DROP TABLE IF EXISTS companies CASCADE;")
    op.execute("DROP TABLE IF EXISTS products CASCADE;") 
    op.execute("DROP TABLE IF EXISTS communes CASCADE;")
```

## Phase 3: Redis Cache with TTL & Invalidation

### Step 6 - Redis Client
```python
# app/cache/redis_client.py
import aioredis
import json
import structlog
from typing import Any, Optional, Callable
from app.config import settings

logger = structlog.get_logger()
redis: aioredis.Redis = None

async def init_redis():
    global redis
    try:
        redis = aioredis.from_url(
            settings.redis_url, 
            encoding="utf-8", 
            decode_responses=True,
            socket_timeout=settings.redis_timeout
        )
        await redis.ping()
        logger.info("Redis connection initialized")
    except Exception as e:
        logger.error("Failed to initialize Redis", error=str(e))
        raise

async def close_redis():
    global redis
    if redis:
        await redis.close()
        logger.info("Redis connection closed")

async def get_or_set_cache(
    key: str, 
    query_func: Callable, 
    ttl: int = settings.cache_ttl,
    *args, **kwargs
) -> Any:
    try:
        cached = await redis.get(key)
        if cached:
            logger.debug("Cache hit", key=key)
            return json.loads(cached)
        
        logger.debug("Cache miss", key=key)
        result = await query_func(*args, **kwargs)
        await redis.set(key, json.dumps(result, default=str), ex=ttl)
        return result
    except Exception as e:
        logger.error("Cache operation failed", key=key, error=str(e))
        # Fallback to direct query
        return await query_func(*args, **kwargs)

async def invalidate_cache(*keys: str):
    try:
        if keys:
            await redis.delete(*keys)
            logger.info("Cache invalidated", keys=keys)
    except Exception as e:
        logger.error("Cache invalidation failed", keys=keys, error=str(e))
```

## Phase 4: Pydantic Models & Error Handling

### Step 7 - Response Models
```python
# app/models/responses.py
from pydantic import BaseModel, EmailStr
from typing import Optional, List
from uuid import UUID
from datetime import datetime

class ProductOut(BaseModel):
    uuid: UUID
    name_es: str
    name_en: str
    created_at: datetime

class CommuneOut(BaseModel):
    uuid: UUID
    name: str
    created_at: datetime

class CompanyOut(BaseModel):
    uuid: UUID
    name: str
    description_es: Optional[str]
    description_en: Optional[str]
    address: Optional[str]
    phone: Optional[str]
    email: Optional[EmailStr]
    image_url: Optional[str]
    created_at: datetime
    updated_at: datetime

class PaginatedResponse(BaseModel):
    items: List[CompanyOut]
    total: int
    page: int
    page_size: int
    total_pages: int

# app/models/errors.py
class ErrorResponse(BaseModel):
    error: str
    details: Optional[str] = None
    code: str
    timestamp: datetime
```

### Step 8 - Raw SQL Queries with Parameters
```python
# app/database/queries.py
import asyncpg
import structlog
from typing import List, Dict, Any

logger = structlog.get_logger()

async def fetch_products(db: asyncpg.Connection) -> List[Dict[str, Any]]:
    query = """
        SELECT uuid, name_es, name_en, created_at 
        FROM products 
        WHERE deleted_at IS NULL 
        ORDER BY name_es
    """
    rows = await db.fetch(query)
    return [dict(row) for row in rows]

async def fetch_communes(db: asyncpg.Connection) -> List[Dict[str, Any]]:
    query = """
        SELECT uuid, name, created_at 
        FROM communes 
        WHERE deleted_at IS NULL 
        ORDER BY name
    """
    rows = await db.fetch(query)
    return [dict(row) for row in rows]

async def fetch_companies_paginated(
    db: asyncpg.Connection, 
    page: int = 1, 
    page_size: int = 10,
    product_uuid: str = None,
    commune_uuid: str = None,
    search: str = None
) -> Dict[str, Any]:
    offset = (page - 1) * page_size
    
    # Build WHERE conditions
    conditions = ["deleted_at IS NULL"]
    params = []
    param_count = 0
    
    if product_uuid:
        param_count += 1
        conditions.append(f"product_uuid = ${param_count}")
        params.append(product_uuid)
    
    if commune_uuid:
        param_count += 1
        conditions.append(f"commune_uuid = ${param_count}")
        params.append(commune_uuid)
    
    if search:
        param_count += 1
        conditions.append(f"name ILIKE ${param_count}")
        params.append(f"%{search}%")
    
    where_clause = " AND ".join(conditions)
    
    # Count query
    count_query = f"SELECT COUNT(*) FROM companies WHERE {where_clause}"
    total = await db.fetchval(count_query, *params)
    
    # Data query
    param_count += 1
    limit_param = param_count
    param_count += 1
    offset_param = param_count
    
    data_query = f"""
        SELECT uuid, name, description_es, description_en, 
               address, phone, email, image_url, created_at, updated_at
        FROM companies 
        WHERE {where_clause}
        ORDER BY created_at DESC 
        LIMIT ${limit_param} OFFSET ${offset_param}
    """
    
    params.extend([page_size, offset])
    rows = await db.fetch(data_query, *params)
    
    return {
        "items": [dict(row) for row in rows],
        "total": total,
        "page": page,
        "page_size": page_size,
        "total_pages": (total + page_size - 1) // page_size
    }
```

## Phase 5: Security & Content Moderation

### Step 9 - Content Moderation
```python
# app/utils/content_moderation.py
from better_profanity import profanity
import magic
import structlog
from typing import Optional

logger = structlog.get_logger()

# Load profanity filter
profanity.load_censor_words()

class ContentModerator:
    def __init__(self):
        self.allowed_mime_types = {
            "image/jpeg", "image/png", "image/webp"
        }
    
    def check_text_content(self, text: str) -> tuple[bool, Optional[str]]:
        if not text:
            return True, None
        
        # Check for profanity
        if profanity.contains_profanity(text):
            return False, "Text contains inappropriate content"
        
        # Check for HTML/XSS attempts
        dangerous_patterns = ["<script", "javascript:", "on\w+="]
        text_lower = text.lower()
        for pattern in dangerous_patterns:
            if pattern in text_lower:
                return False, "Text contains potentially harmful content"
        
        return True, None
    
    def check_file_upload(self, file_content: bytes, filename: str) -> tuple[bool, Optional[str]]:
        # Check file size
        if len(file_content) > 10 * 1024 * 1024:  # 10MB
            return False, "File size exceeds limit"
        
        # Check MIME type using magic bytes
        try:
            mime_type = magic.from_buffer(file_content, mime=True)
            if mime_type not in self.allowed_mime_types:
                return False, f"File type {mime_type} not allowed"
        except Exception as e:
            logger.error("MIME type detection failed", error=str(e))
            return False, "Unable to verify file type"
        
        return True, None

moderator = ContentModerator()
```

## Phase 6: FastAPI Application & Middleware

### Step 10 - Middleware Setup
```python
# app/middleware/logging.py
import structlog
import time
import uuid
from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware

class LoggingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # Generate correlation ID
        correlation_id = str(uuid.uuid4())
        request.state.correlation_id = correlation_id
        
        # Log request
        logger = structlog.get_logger()
        start_time = time.time()
        
        logger.info(
            "Request started",
            correlation_id=correlation_id,
            method=request.method,
            url=str(request.url),
            client_ip=request.client.host if request.client else None
        )
        
        # Process request
        response = await call_next(request)
        
        # Log response
        duration = time.time() - start_time
        logger.info(
            "Request completed",
            correlation_id=correlation_id,
            status_code=response.status_code,
            duration_ms=round(duration * 1000, 2)
        )
        
        response.headers["X-Correlation-ID"] = correlation_id
        return response

# app/middleware/security.py
from fastapi import Request, HTTPException
from starlette.middleware.base import BaseHTTPMiddleware

class SecurityMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # Request size limit
        content_length = request.headers.get("content-length")
        if content_length and int(content_length) > 10 * 1024 * 1024:  # 10MB
            raise HTTPException(413, "Request too large")
        
        response = await call_next(request)
        
        # Security headers
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-Frame-Options"] = "DENY"
        response.headers["X-XSS-Protection"] = "1; mode=block"
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
        
        return response
```

### Step 11 - API Endpoints
```python
# app/api/v1/endpoints/health.py
from fastapi import APIRouter, HTTPException, Depends
from app.database.connection import get_db
from app.cache.redis_client import redis
import asyncpg

router = APIRouter()

@router.get("/health")
async def health_check(db: asyncpg.Connection = Depends(get_db)):
    try:
        # Test database
        await db.fetchval("SELECT 1")
        
        # Test Redis
        await redis.ping()
        
        return {
            "status": "healthy",
            "services": {
                "database": "ok",
                "redis": "ok"
            }
        }
    except Exception as e:
        raise HTTPException(503, f"Service unhealthy: {str(e)}")

# app/api/v1/endpoints/products.py
from fastapi import APIRouter, Depends, HTTPException
from typing import List
import asyncpg
from app.database.connection import get_db
from app.database.queries import fetch_products
from app.cache.redis_client import get_or_set_cache
from app.models.responses import ProductOut

router = APIRouter()

@router.get("/products", response_model=List[ProductOut])
async def get_products(db: asyncpg.Connection = Depends(get_db)):
    try:
        return await get_or_set_cache(
            "products_all", 
            fetch_products,
            db=db
        )
    except Exception as e:
        raise HTTPException(500, f"Failed to fetch products: {str(e)}")
```

### Step 12 - Main Application
```python
# app/main.py
import structlog
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from contextlib import asynccontextmanager

from app.config import settings
from app.database.connection import init_db_pool, close_db_pool
from app.cache.redis_client import init_redis, close_redis
from app.middleware.logging import LoggingMiddleware
from app.middleware.security import SecurityMiddleware
from app.api.v1.router import api_router

# Configure structured logging
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.add_log_level,
        structlog.processors.JSONRenderer()
    ],
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await init_db_pool()
    await init_redis()
    yield
    # Shutdown
    await close_db_pool()
    await close_redis()

app = FastAPI(
    title=settings.project_name,
    version="1.0.0",
    lifespan=lifespan,
    docs_url="/docs" if settings.debug else None,
    redoc_url="/redoc" if settings.debug else None
)

# Middleware
app.add_middleware(SecurityMiddleware)
app.add_middleware(LoggingMiddleware)
app.add_middleware(GZipMiddleware, minimum_size=1000)
app.add_middleware(
    TrustedHostMiddleware, 
    allowed_hosts=["localhost", "127.0.0.1", "*.yourdomain.com"]
)
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)

# Routes
app.include_router(api_router, prefix=settings.api_v1_prefix)

@app.get("/")
async def root():
    return {"message": "API is running", "version": "1.0.0"}
```

## Phase 7: Testing

### Step 13 - Test Configuration
```python
# app/tests/conftest.py
import pytest
import asyncio
from httpx import AsyncClient
from app.main import app

@pytest.fixture(scope="session")
def event_loop():
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture
async def client():
    async with AsyncClient(app=app, base_url="http://test") as ac:
        yield ac

# app/tests/test_endpoints.py
import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_health_endpoint(client: AsyncClient):
    response = await client.get("/api/v1/health")
    assert response.status_code == 200
    data = response.json()
    assert data["status"] == "healthy"

@pytest.mark.asyncio
async def test_products_endpoint(client: AsyncClient):
    response = await client.get("/api/v1/products")
    assert response.status_code == 200
    assert isinstance(response.json(), list)

@pytest.mark.asyncio
async def test_request_size_limit(client: AsyncClient):
    large_data = {"data": "x" * (11 * 1024 * 1024)}  # 11MB
    response = await client.post("/api/v1/companies", json=large_data)
    assert response.status_code == 413
```

## Phase 8: Development Tools

### Step 14 - Pre-commit Hooks
```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/psf/black
    rev: 23.11.0
    hooks:
      - id: black
  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort
  - repo: https://github.com/pycqa/flake8
    rev: 6.1.0
    hooks:
      - id: flake8
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.7.1
    hooks:
      - id: mypy
```

### Step 15 - Makefile
```makefile
# Makefile
.PHONY: install dev test lint format migrate run docker-build docker-run

install:
	pip install -r requirements.txt

dev:
	pip install -r requirements-dev.txt
	pre-commit install

test:
	pytest app/tests/ -v

lint:
	flake8 app/
	mypy app/

format:
	black app/
	isort app/

migrate:
	alembic upgrade head

run-dev:
	uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

run-prod:
	gunicorn app.main:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000

docker-build:
	docker build -t my-demo-app .

docker-run:
	docker-compose up -d
```

## Phase 9: Dockerization

### Step 16 - Dockerfile
```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    libpq-dev \
    libmagic1 \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY app/ ./app/
COPY alembic/ ./alembic/
COPY alembic.ini .

# Create non-root user
RUN useradd --create-home --shell /bin/bash appuser
RUN chown -R appuser:appuser /app
USER appuser

EXPOSE 8000

CMD ["gunicorn", "app.main:app", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8000"]
```

### Step 17 - Docker Compose
```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://demo:demo123@postgres:5432/demo_db
      - REDIS_URL=redis://redis:6379
    depends_on:
      - postgres
      - redis
    volumes:
      - ./app:/app/app
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

  postgres:
    image: postgres:15
    environment:
      - POSTGRES_USER=demo
      - POSTGRES_PASSWORD=demo123
      - POSTGRES_DB=demo_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

volumes:
  postgres_data:
  redis_data:
```

## Phase 10: Deployment

### Step 18 - Environment Files
```bash
# .env.example
DATABASE_URL=postgresql://user:password@localhost:5432/demo_db
REDIS_URL=redis://localhost:6379
SECRET_KEY=your-secret-key-here
DEBUG=true
ALLOWED_ORIGINS=["http://localhost:3000"]
```

### Step 19 - Deployment Scripts
```bash
#!/bin/bash
# scripts/deploy.sh

set -e

echo "ðŸš€ Starting deployment..."

# Pull latest code
git pull origin main

# Build and run with Docker Compose
docker-compose down
docker-compose build
docker-compose up -d

# Run migrations
docker-compose exec api alembic upgrade head

# Health check
sleep 10
curl -f http://localhost:8000/api/v1/health || exit 1

echo "âœ… Deployment completed successfully!"
```

## Step-by-Step Execution

### Phase 1: Setup (Day 1)
```bash
mkdir my_demo_app && cd my_demo_app
# Create all directories and files from Step 1
pip install -r requirements.txt
pip install -r requirements-dev.txt
pre-commit install
```

### Phase 2: Database (Day 1)
```bash
# Setup Postgres locally or via Docker
alembic init alembic
# Create migration file from Step 5
alembic upgrade head
```

### Phase 3: Development (Day 2)
```bash
# Implement all modules from Steps 6-12
uvicorn app.main:app --reload
# Test endpoints at http://localhost:8000/docs
```

### Phase 4: Testing (Day 2)
```bash
pytest app/tests/ -v
# Fix any failing tests
```

### Phase 5: Polish (Day 3)
```bash
make format
make lint
# Fix any linting issues
```

### Phase 6: Deployment (Day 3)
```bash
docker-compose up -d
# Test production setup
./scripts/deploy.sh
```

This gives you a production-ready FastAPI backend that demonstrates professional development practices and won't look like "copy-paste LLM code."